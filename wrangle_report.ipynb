{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This data wrangling project is about a twitter account @WeRateDogs. This account gives ratings to people's dogs with a humorous comment about the dog. The account was started in 2015 by college student Matt Nelson, and has received international media attention for its popularity. WeRateDogs asks people to send photos of their dogs, then tweets selected photos rating and a humorous comment. Dogs are rated on a scale of one to ten, but are invariably given ratings in excess of the maximum, such as \"13/10\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Data Gathering\n",
    "The data sets for this project was from the archive of @WeRateDogs twitter account with username @dog_rates. This archive contains basic tweet data (tweet ID, timestamp, ratings, text content, tweet source, etc.). The data set contain 2356 tweet data and 17 columns. There are 3 data sets involved in this project, twitter_archive_enhanced.csv, image_predictions.csv, and another data set gathered from Twitter's API.\n",
    "\n",
    "## Twitter archive csv file\n",
    "The twitter_archive_enhanced.csv was provided by Udacity via a provided link. The file was downloaded manually and uploaded to Jupyter notebook. The csv file was read into pandas dataframe tweet_archive_df.\n",
    "\n",
    "## Image predictions csv file\n",
    "The data set was hosted on Udacity's servers and was downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv. The file was read into pandas dataframe image_predictions.\n",
    "\n",
    "## Twitter API data\n",
    "The tweet IDs in the WeRateDogs Twitter archive was used to query the Twitter API for each tweet's JSON data using Python's Tweepy library, the tweet's entire set of JSON data was stored in a file called tweet_json.txt file. The tweet's JSON data was written to its own line. Then the .txt file was read line by line into a pandas DataFrame with tweet ID, retweet count, and favorite count. The file was read into pandas dataframe tweet_count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Data Assessment\n",
    "I was able to identify 8 quality issues and 4 tidiness issues using the visual and programmatic assessment.\n",
    "\n",
    "## Quality\n",
    "- I was able to identify that the source column contained html characters by using the .head() function to view the first five rows of the tweet_archive_df dataframe.\n",
    "- I used value_counts() function on the ratings_denominator column and realized there are other values other than 10.\n",
    "- I used the .info() function to check for null values and data types and timestamp column should be converted to datetime.\n",
    "- The rating_denominator column has values less than 10 and values greater than 10.\n",
    "- The value_counts function in the name column shows that it needs to be cleaned up because it contains words that are not names, such as, officially, infuriating, getting, etc.\n",
    "- The number of tweet id's in the tweet_id column of the twitter archive data set is more than the number of tweet_id's in tweet_id column of the image prediction data set.\n",
    "- There is completeness issue in the columns relating to in_reply in the twitter archive data set.\n",
    "- There are rows with conflicting stages, i.e., more than one stage.\n",
    "\n",
    "## Tidiness\n",
    "- The dog stage columns should be merged into one column.\n",
    "- A new column should be added for the various dog breeds.\n",
    "- Merge data frames together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Data Cleaning\n",
    "Since, most of the issues were in the tweet_archive_df table, I made a copy of the table and performed the necessary cleaning needed to address the quality and tidiness issues. These issues were addressed by defining the issue, writing a code to solve the issues, and testing if the issue was resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data\n",
    "After performing the three step above, i stored the data in the twitter_archive_master csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
